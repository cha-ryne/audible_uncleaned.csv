{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a97219",
   "metadata": {},
   "source": [
    "### Install Necessary Libraries\n",
    "In this step, we are installing the required libraries for data manipulation, visualization, and cleaning:\n",
    "- **pandas**: For data manipulation and analysis\n",
    "- **numpy**: For numerical operations\n",
    "- **matplotlib**: For visualizations\n",
    "- **seaborn**: For statistical data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07419f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391fd454",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "We import all necessary libraries for data manipulation, analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95491dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Set display options for better output formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9f5ba",
   "metadata": {},
   "source": [
    "### Load the Dataset\n",
    "Load the raw Audible dataset to begin our cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6529c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the specified path\n",
    "data = pd.read_csv('audible_uncleaned.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43def72d",
   "metadata": {},
   "source": [
    "### 2.1 Identify Missing Values\n",
    "Before cleaning, we need to understand what data quality issues exist in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values in the dataset\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Display missing values count per column\n",
    "print(\"Missing Values per Column:\")\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")\n",
    "print(f\"Percentage of missing data: {(missing_values.sum() / (data.shape[0] * data.shape[1])) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf10e2",
   "metadata": {},
   "source": [
    "### 2.2 Detect Duplicate Rows\n",
    "Duplicate entries can skew our analysis, so we need to identify and handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88257d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect duplicate rows\n",
    "duplicates = data.duplicated().sum()\n",
    "\n",
    "# Display the number of duplicate rows\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "print(f\"Percentage of duplicates: {(duplicates / len(data)) * 100:.2f}%\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"\\nSample duplicate rows:\")\n",
    "    print(data[data.duplicated(keep=False)].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd05c4",
   "metadata": {},
   "source": [
    "### 2.3 Check Data Types\n",
    "Examining data types helps us identify columns that need type conversion or formatting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f08eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types of all columns to spot any inconsistent data formats\n",
    "print(\"Data Types per Column:\")\n",
    "print(data.dtypes)\n",
    "print(\"\\nSample data from each column:\")\n",
    "for col in data.columns:\n",
    "    print(f\"\\n{col}: {data[col].dropna().iloc[0] if not data[col].dropna().empty else 'No data'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e99211",
   "metadata": {},
   "source": [
    "### 3.1 Remove Duplicate Rows\n",
    "Remove all duplicate rows while keeping the first occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store initial count for comparison\n",
    "initial_rows = len(data)\n",
    "\n",
    "# Remove duplicate rows\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Calculate rows removed\n",
    "rows_removed = initial_rows - len(data)\n",
    "\n",
    "print(f\"Number of duplicate rows removed: {rows_removed}\")\n",
    "print(f\"Dataset shape after removing duplicates: {data.shape}\")\n",
    "print(\"\\nDataset after removing duplicates:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4302bbcc",
   "metadata": {},
   "source": [
    "### 3.2 Standardize Language Column\n",
    "Capitalize the first letter of each word in the 'language' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalize the first letter of each word in the 'language' column\n",
    "data['language'] = data['language'].str.capitalize()\n",
    "\n",
    "# Display the updated 'language' column\n",
    "print(\"Updated 'language' column:\")\n",
    "print(data['language'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72049502",
   "metadata": {},
   "source": [
    "### 3.3 Clean Author and Narrator Names\n",
    "Create a function to standardize name formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a space between the first and last name and after the comma\n",
    "def format_names(name):\n",
    "    \"\"\"\n",
    "    Clean and format names by:\n",
    "    1. Adding space after commas\n",
    "    2. Adding space between CamelCase words (FirstnameLastname -> Firstname Lastname)\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    \n",
    "    # Add a space after the comma\n",
    "    name = name.replace(\",\", \", \")\n",
    "    \n",
    "    # Insert space between first and last name by capitalizing where the uppercase starts\n",
    "    # Using regex to insert space before each uppercase letter that follows a lowercase letter\n",
    "    name = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', name)\n",
    "    \n",
    "    return name\n",
    "\n",
    "# Apply the function to the 'author' column\n",
    "print(\"Original author format:\", data['author'].iloc[0])\n",
    "data['author'] = data['author'].apply(format_names)\n",
    "print(\"Formatted author:\", data['author'].iloc[0])\n",
    "\n",
    "print(\"\\nUpdated 'author' column with formatted names:\")\n",
    "print(data[['author']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c424fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same function to the 'narrator' column\n",
    "print(\"Original narrator format:\", data['narrator'].iloc[0])\n",
    "data['narrator'] = data['narrator'].apply(format_names)\n",
    "print(\"Formatted narrator:\", data['narrator'].iloc[0])\n",
    "\n",
    "print(\"\\nUpdated 'narrator' column with formatted names:\")\n",
    "print(data[['narrator']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f563361",
   "metadata": {},
   "source": [
    "### 3.4 Remove Prefixes from Author and Narrator Columns\n",
    "Strip prefixes and clean up extra whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59648618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Writtenby:' from the 'author' column\n",
    "data['author'] = data['author'].str.replace('Writtenby:', '', regex=False)\n",
    "\n",
    "# Remove 'Narratedby:' from the 'narrator' column  \n",
    "data['narrator'] = data['narrator'].str.replace('Narratedby:', '', regex=False)\n",
    "\n",
    "# Clean up any extra spaces\n",
    "data['author'] = data['author'].str.strip()\n",
    "data['narrator'] = data['narrator'].str.strip()\n",
    "\n",
    "# Display the cleaned 'author' and 'narrator' columns\n",
    "print(\"Cleaned 'author' and 'narrator' columns:\")\n",
    "print(data[['author', 'narrator']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff05473",
   "metadata": {},
   "source": [
    "### 3.5 Convert Time Format to Minutes\n",
    "Convert all time values to total minutes for consistent numerical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_to_minutes(time_str):\n",
    "    \"\"\"\n",
    "    Convert time format from \"X hrs and Y mins\" to total minutes.\n",
    "    Handles various formats:\n",
    "    - \"2 hrs and 20 mins\" -> 140 minutes\n",
    "    - \"1 hr and 30 mins\" -> 90 minutes\n",
    "    - \"45 mins\" -> 45 minutes\n",
    "    \"\"\"\n",
    "    if pd.isna(time_str):\n",
    "        return 0\n",
    "    \n",
    "    # Use regex to extract hours and minutes\n",
    "    hours = 0\n",
    "    minutes = 0\n",
    "    time_parts = time_str.split(' and ')\n",
    "    \n",
    "    for part in time_parts:\n",
    "        if 'hr' in part:\n",
    "            hours_match = re.search(r'(\\d+)', part)\n",
    "            if hours_match:\n",
    "                hours = int(hours_match.group())\n",
    "        elif 'min' in part:\n",
    "            minutes_match = re.search(r'(\\d+)', part)\n",
    "            if minutes_match:\n",
    "                minutes = int(minutes_match.group())\n",
    "    \n",
    "    # Calculate total minutes\n",
    "    total_minutes = (hours * 60) + minutes\n",
    "    return total_minutes\n",
    "\n",
    "# Show before and after conversion\n",
    "print(\"Before conversion:\")\n",
    "print(data['time'].head())\n",
    "\n",
    "# Apply the function to replace 'time' column with total minutes\n",
    "data['time'] = data['time'].apply(convert_time_to_minutes)\n",
    "\n",
    "print(\"\\nAfter conversion to minutes:\")\n",
    "print(data['time'].head())\n",
    "\n",
    "# Check the data types and the updated 'time' column\n",
    "print(f\"\\nTime column statistics:\")\n",
    "print(f\"Average duration: {data['time'].mean():.1f} minutes ({data['time'].mean()/60:.1f} hours)\")\n",
    "print(f\"Range: {data['time'].min()} - {data['time'].max()} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9f115",
   "metadata": {},
   "source": [
    "### 3.6 Convert Release Date to Proper DateTime Format\n",
    "Convert the releasedate column from string to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85513fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original format\n",
    "print(\"Original release date format:\")\n",
    "print(data['releasedate'].head())\n",
    "\n",
    "# Convert 'releasedate' to datetime format\n",
    "data['releasedate'] = pd.to_datetime(data['releasedate'], errors='coerce')\n",
    "\n",
    "print(\"\\nAfter datetime conversion:\")\n",
    "print(data['releasedate'].head())\n",
    "print(f\"\\nData type: {data['releasedate'].dtype}\")\n",
    "\n",
    "# Check for any conversion errors\n",
    "conversion_errors = data['releasedate'].isna().sum()\n",
    "print(f\"Conversion errors (NaT values): {conversion_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8a0eb",
   "metadata": {},
   "source": [
    "### 4.1 Extract Star Ratings and Number of Ratings\n",
    "Use regex to extract star rating and number of ratings into separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa95bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Rename the original 'stars' column to preserve the source text\n",
    "data.rename(columns={'stars': 'stars_original_text'}, inplace=True, errors='ignore')\n",
    "\n",
    "# Step 2: Ensure the source text column is a string type\n",
    "data['stars_original_text'] = data['stars_original_text'].astype(str)\n",
    "\n",
    "print(\"Original stars format:\")\n",
    "print(data['stars_original_text'].head())\n",
    "\n",
    "# Step 3: Define the regular expression pattern for the format \"X out of 5 starsY ratings\"\n",
    "pattern = r'(?P<star_rating>\\d\\.?\\d*)\\s*out of 5 stars(?P<num_ratings>\\d+)\\s*ratings'\n",
    "\n",
    "# Step 4: Use .str.extract() to pull the data into new columns based on the pattern\n",
    "extracted_data = data['stars_original_text'].str.extract(pattern)\n",
    "\n",
    "# Step 5: Create the new columns and convert them to numeric types\n",
    "data['star_rating'] = pd.to_numeric(extracted_data['star_rating'], errors='coerce')\n",
    "data['num_ratings'] = pd.to_numeric(extracted_data['num_ratings'], errors='coerce')\n",
    "\n",
    "# Step 6: Replace any resulting empty (NaN) values in the new columns with 0\n",
    "data.fillna({'star_rating': 0, 'num_ratings': 0}, inplace=True)\n",
    "\n",
    "# Step 7: Convert 'num_ratings' to a whole number (integer)\n",
    "data['num_ratings'] = data['num_ratings'].astype(int)\n",
    "\n",
    "print(\"\\nData Transformation Result:\")\n",
    "print(data[['stars_original_text', 'star_rating', 'num_ratings']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd4fd8",
   "metadata": {},
   "source": [
    "### 4.2 Finalize Star Rating Columns\n",
    "Round star ratings and rename columns for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3cec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the star_rating column to one decimal place\n",
    "data['star_rating'] = data['star_rating'].round(1)\n",
    "\n",
    "# Rename the final columns for clarity\n",
    "data.rename(columns={'star_rating': 'stars', 'num_ratings': 'ratings'}, inplace=True)\n",
    "\n",
    "# Display the result\n",
    "print(\"Final stars and ratings columns:\")\n",
    "print(data[['stars', 'ratings']].head())\n",
    "\n",
    "# Show statistics\n",
    "print(f\"\\nStar ratings statistics:\")\n",
    "print(f\"Average rating: {data['stars'].mean():.2f}\")\n",
    "print(f\"Rating distribution:\")\n",
    "print(data['stars'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d43966",
   "metadata": {},
   "source": [
    "### 5.1 Clean and Standardize Price Data\n",
    "Remove non-numeric characters and convert to proper numeric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c2a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original price format:\")\n",
    "print(data['price'].head())\n",
    "\n",
    "# Remove any non-numeric characters (e.g., commas, dollar signs)\n",
    "data['price'] = data['price'].astype(str).str.replace(r'[^\\d.]', '', regex=True)\n",
    "\n",
    "# Convert the price column to numeric\n",
    "data['price'] = pd.to_numeric(data['price'], errors='coerce')\n",
    "\n",
    "# Handle any NaN values that might result from conversion\n",
    "data['price'] = data['price'].fillna(0)\n",
    "\n",
    "# Round the 'price' column to 2 decimal places\n",
    "data['price'] = data['price'].round(2)\n",
    "\n",
    "print(\"\\nCleaned price column:\")\n",
    "print(data['price'].head())\n",
    "\n",
    "# Show price statistics\n",
    "print(f\"\\nPrice statistics:\")\n",
    "print(f\"Average price: ${data['price'].mean():.2f}\")\n",
    "print(f\"Price range: ${data['price'].min():.2f} - ${data['price'].max():.2f}\")\n",
    "print(f\"Median price: ${data['price'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc931b2",
   "metadata": {},
   "source": [
    "### 6.1 Remove Unnecessary Columns and Rename for Clarity\n",
    "Drop the original text column and rename columns for better readability.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e68944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'stars_original_text' column as it's no longer needed\n",
    "data = data.drop(columns=['stars_original_text'])\n",
    "\n",
    "# Rename columns for better clarity and consistency\n",
    "data.rename(columns={\n",
    "    'time': 'time_minutes', \n",
    "    'releasedate': 'release_date'\n",
    "}, inplace=True)\n",
    "\n",
    "print(\"Final dataset structure:\")\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "\n",
    "# Display the final cleaned dataset\n",
    "print(\"\\nFinal cleaned dataset:\")\n",
    "data.head()\n",
    "\n",
    "print(\"Checking for duplicates created during cleaning...\")\n",
    "duplicates_after_cleaning = data.duplicated().sum()\n",
    "if duplicates_after_cleaning > 0:\n",
    "    print(f\"Found {duplicates_after_cleaning} duplicates created during cleaning\")\n",
    "    data = data.drop_duplicates()\n",
    "    print(f\"Removed duplicates. New shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc005a8",
   "metadata": {},
   "source": [
    "### 7.1 Final Data Quality Check\n",
    "Perform comprehensive quality checks on the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b03c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FINAL DATA QUALITY REPORT ===\")\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n1. Missing Values:\")\n",
    "missing_final = data.isnull().sum()\n",
    "print(missing_final[missing_final > 0] if missing_final.sum() > 0 else \"No missing values!\")\n",
    "\n",
    "print(\"\\n2. Duplicate Rows:\")\n",
    "duplicates_final = data.duplicated().sum()\n",
    "print(f\"No duplicates!\" if duplicates_final == 0 else f\"{duplicates_final} duplicates found\")\n",
    "\n",
    "print(\"\\n3. Data Types:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "print(\"\\n4. Data Summary Statistics:\")\n",
    "print(data.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff52f1",
   "metadata": {},
   "source": [
    "### 7.2 Save Cleaned Dataset\n",
    "Export the cleaned dataset to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "output_filename = 'audible_cleaned_final.csv'\n",
    "data.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved as '{output_filename}'\")\n",
    "print(f\"Final dataset: {data.shape[0]:,} rows × {data.shape[1]} columns\")\n",
    "\n",
    "# Create a summary of cleaning operations performed\n",
    "cleaning_summary = {\n",
    "    'Operation': [\n",
    "        'Removed duplicate rows',\n",
    "        'Standardized language capitalization', \n",
    "        'Formatted author/narrator names',\n",
    "        'Removed text prefixes',\n",
    "        'Converted time to minutes',\n",
    "        'Converted dates to datetime',\n",
    "        'Extracted star ratings',\n",
    "        'Cleaned price format',\n",
    "        'Renamed columns for clarity'\n",
    "    ],\n",
    "    'Impact': [\n",
    "        f'{rows_removed} rows removed',\n",
    "        'Consistent capitalization',\n",
    "        'Proper name spacing',\n",
    "        'Clean text fields',\n",
    "        'Numerical time analysis ready',\n",
    "        'Date analysis ready', \n",
    "        'Separate rating metrics',\n",
    "        'Numerical price analysis ready',\n",
    "        'Improved readability'\n",
    "    ]\n",
    "}\n",
    "\n",
    "cleaning_df = pd.DataFrame(cleaning_summary)\n",
    "print(\"\\n=== CLEANING OPERATIONS SUMMARY ===\")\n",
    "print(cleaning_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48bd12f",
   "metadata": {},
   "source": [
    "### 8.1 Language Distribution Analysis\n",
    "Create a pie chart showing the distribution of audiobooks by language, grouping smaller languages into \"Other Languages\" for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Count number of audiobooks per language\n",
    "language_counts = data['language'].value_counts()\n",
    "\n",
    "# Compute total count for percentage calculation\n",
    "total = language_counts.sum()\n",
    "\n",
    "print(\"Language distribution (raw counts):\")\n",
    "print(language_counts)\n",
    "\n",
    "# Separate languages with ≥1% and group others\n",
    "threshold = 0.01  # 1%\n",
    "main_languages = language_counts[language_counts / total >= threshold]\n",
    "other_languages = language_counts[language_counts / total < threshold]\n",
    "\n",
    "print(f\"\\nLanguages representing ≥1% of data: {len(main_languages)}\")\n",
    "print(f\"Languages grouped as 'Other': {len(other_languages)}\")\n",
    "\n",
    "# Add \"Other Languages\" as a merged entry\n",
    "language_counts_cleaned = main_languages.copy()\n",
    "if len(other_languages) > 0:\n",
    "    language_counts_cleaned['Other Languages'] = other_languages.sum()\n",
    "\n",
    "# Plot updated pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(language_counts_cleaned, \n",
    "        labels=language_counts_cleaned.index, \n",
    "        autopct='%1.1f%%', \n",
    "        startangle=140,\n",
    "        colors=plt.cm.Set3.colors)\n",
    "plt.title('Distribution of Audiobooks by Language', fontsize=16, fontweight='bold')\n",
    "plt.axis('equal')  # Ensures the pie is circular\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nInsight: The dataset contains audiobooks in {len(language_counts)} different languages\")\n",
    "print(f\"Market Focus: {language_counts.index[0]} dominates with {(language_counts.iloc[0]/total)*100:.1f}% of all audiobooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0486556",
   "metadata": {},
   "source": [
    "### 8.2 Top Authors by Average Rating Analysis\n",
    "Analyze and visualize the top 10 authors based on their average ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42235b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Compute and sort average number of ratings per author\n",
    "author_ratings = data.groupby('author')['ratings'].mean().sort_values(ascending=False).head(10).round(2)\n",
    "author_df = author_ratings.reset_index()  # Convert to DataFrame for plotting\n",
    "\n",
    "print(\"Top 10 Authors by Average Number of Ratings (Popularity):\")\n",
    "print(author_df)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    data=author_df,\n",
    "    x='ratings',              \n",
    "    y='author',               \n",
    "    hue='author',             # Assign hue to match palette with y-values\n",
    "    palette='viridis',        \n",
    "    legend=False              # Hide legend since author names are already on y-axis\n",
    ")\n",
    "\n",
    "# Add Add text labels to bars\n",
    "for index, value in enumerate(author_df['ratings']):\n",
    "    plt.text(value + 0.01, index, str(value), va='center')\n",
    "\n",
    "# Customize plot appearance\n",
    "plt.title('Top 10 Authors by Average Number of Ratings (Popularity)', fontsize=16)\n",
    "plt.xlabel('Average Number of Ratings per Book', fontsize=12)\n",
    "plt.ylabel('Author', fontsize=12)\n",
    "plt.tight_layout()  # Adjust layout to prevent label cutoff\n",
    "plt.show()\n",
    "\n",
    "# Display summary insights\n",
    "total_authors = data['author'].nunique()\n",
    "avg_popularity = author_df['ratings'].mean()\n",
    "print(f\"\\nPopularity Analysis Summary:\")\n",
    "print(f\"Total unique authors in dataset: {total_authors}\")\n",
    "print(f\"Average popularity among top 10: {avg_popularity:.1f} ratings per book\")\n",
    "print(f\"Most popular author: {author_df.iloc[0]['author']} ({author_df.iloc[0]['ratings']} avg ratings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274e461",
   "metadata": {},
   "source": [
    "### 8.3 Price Trend Analysis Over Time\n",
    "Analyze and visualize the average price trends by release year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2668aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure price is properly cleaned and numeric\n",
    "print(\"Cleaning price data for trend analysis...\")\n",
    "print(f\"Current price data type: {data['price'].dtype}\")\n",
    "\n",
    "# Additional price cleaning if needed\n",
    "data['price'] = data['price'].astype(str)\n",
    "\n",
    "# Remove currency symbols and commas using a raw string\n",
    "data['price'] = data['price'].str.replace(r'[\\₱$,]', '', regex=True)\n",
    "\n",
    "# Convert to numeric\n",
    "data['price'] = pd.to_numeric(data['price'], errors='coerce')\n",
    "\n",
    "# Handle any NaN values\n",
    "price_na_count = data['price'].isna().sum()\n",
    "if price_na_count > 0:\n",
    "    print(f\"Warning: {price_na_count} price values could not be converted to numeric\")\n",
    "    data['price'] = data['price'].fillna(data['price'].median())\n",
    "\n",
    "print(f\"Price range after cleaning: ${data['price'].min():.2f} - ${data['price'].max():.2f}\")\n",
    "\n",
    "# Extract year from release date\n",
    "data['release_year'] = data['release_date'].dt.year\n",
    "\n",
    "# Remove any invalid years\n",
    "data = data.dropna(subset=['release_year'])\n",
    "data['release_year'] = data['release_year'].astype(int)\n",
    "\n",
    "print(f\"Year range: {data['release_year'].min()} - {data['release_year'].max()}\")\n",
    "\n",
    "# Group by year and calculate average price\n",
    "price_trend = data.groupby('release_year')['price'].mean().reset_index()\n",
    "price_trend['price'] = price_trend['price'].round(2)\n",
    "\n",
    "print(\"\\nAverage Price by Year:\")\n",
    "print(price_trend.tail(10))  # Show last 10 years\n",
    "\n",
    "# Plot line graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=price_trend, x='release_year', y='price', marker='o', linewidth=2, markersize=6)\n",
    "plt.title('Average Audiobook Price Over Time', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Release Year', fontsize=12)\n",
    "plt.ylabel('Average Price ($)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Price trend insights\n",
    "price_change = price_trend.iloc[-1]['price'] - price_trend.iloc[0]['price']\n",
    "print(f\"\\nPrice Trend Insights:\")\n",
    "print(f\"Price change from {price_trend.iloc[0]['release_year']} to {price_trend.iloc[-1]['release_year']}: ${price_change:.2f}\")\n",
    "print(f\"Recent 5-year average: ${recent_avg:.2f}\")\n",
    "print(f\"Overall average: ${overall_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90961b",
   "metadata": {},
   "source": [
    "### 8.4 Audiobook Release Trends Over Time\n",
    "Analyze the number of audiobook releases per year to identify growth trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc93df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure we have clean column names (if not already done)\n",
    "print(\"Current columns:\", data.columns.tolist())\n",
    "\n",
    "# Verify release_date column exists and is properly formatted\n",
    "if 'release_date' in data.columns:\n",
    "    print(f\"Release date column type: {data['release_date'].dtype}\")\n",
    "    print(f\"Sample release dates: {data['release_date'].dropna().head().tolist()}\")\n",
    "else:\n",
    "    print(\"Warning: release_date column not found\")\n",
    "\n",
    "# Convert release_date to datetime if not already done\n",
    "data['release_date'] = pd.to_datetime(data['release_date'], errors='coerce')\n",
    "\n",
    "# Extract year from release_date (if not already done)\n",
    "data['release_year'] = data['release_date'].dt.year\n",
    "\n",
    "# Remove rows with invalid dates\n",
    "initial_count = len(data)\n",
    "data = data.dropna(subset=['release_year'])\n",
    "final_count = len(data)\n",
    "print(f\"Removed {initial_count - final_count} rows with invalid dates\")\n",
    "\n",
    "# Group by release_year and count entries\n",
    "release_trend = data.groupby('release_year').size().reset_index(name='release_count')\n",
    "\n",
    "print(\"\\nAudiobook Releases by Year:\")\n",
    "print(release_trend.tail(10))  # Show last 10 years\n",
    "\n",
    "# Calculate growth metrics\n",
    "total_releases = release_trend['release_count'].sum()\n",
    "years_span = release_trend['release_year'].max() - release_trend['release_year'].min()\n",
    "avg_per_year = total_releases / len(release_trend)\n",
    "\n",
    "print(f\"\\nRelease Statistics:\")\n",
    "print(f\"Total releases tracked: {total_releases}\")\n",
    "print(f\"Years covered: {years_span} years ({release_trend['release_year'].min()}-{release_trend['release_year'].max()})\")\n",
    "print(f\"Average releases per year: {avg_per_year:.1f}\")\n",
    "\n",
    "# Plotting the trend\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=release_trend, x='release_year', y='release_count', \n",
    "             marker='o', color='royalblue', linewidth=2, markersize=6)\n",
    "\n",
    "plt.title('Trends of Audiobook Releases Over the Years', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Release Year', fontsize=12)\n",
    "plt.ylabel('Number of Audiobook Releases', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Highlight peak years\n",
    "peak_year = release_trend.loc[release_trend['release_count'].idxmax()]\n",
    "plt.annotate(f'Peak: {peak_year[\"release_count\"]} releases\\nin {int(peak_year[\"release_year\"])}',\n",
    "             xy=(peak_year['release_year'], peak_year['release_count']),\n",
    "             xytext=(peak_year['release_year']-2, peak_year['release_count']+5),\n",
    "             arrowprops=dict(arrowstyle='->', color='red', alpha=0.7),\n",
    "             fontsize=10, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Release trend insights\n",
    "recent_5_years = release_trend[release_trend['release_year'] >= release_trend['release_year'].max() - 4]\n",
    "recent_avg = recent_5_years['release_count'].mean()\n",
    "early_5_years = release_trend[release_trend['release_year'] <= release_trend['release_year'].min() + 4]\n",
    "early_avg = early_5_years['release_count'].mean()\n",
    "\n",
    "print(f\"\\nGrowth Analysis:\")\n",
    "print(f\"Peak year: {int(peak_year['release_year'])} with {peak_year['release_count']} releases\")\n",
    "print(f\"Recent 5-year average: {recent_avg:.1f} releases/year\")\n",
    "print(f\"Early 5-year average: {early_avg:.1f} releases/year\")\n",
    "if recent_avg > early_avg:\n",
    "    growth_rate = ((recent_avg - early_avg) / early_avg) * 100\n",
    "    print(f\"Market growth: {growth_rate:.1f}% increase in recent years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the Cleaned Dataset as a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to a CSV file\n",
    "data.to_csv('C:/Users/tresh/Desktop/ANLYTC1_FinalProject_AudibleCleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
